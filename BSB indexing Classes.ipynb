{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**CollectionStats class**\n",
        "Implement the appropirate methods for the CollectionStats class, so that the statistics returned by the corresponding methods that are already implemented returns the correct values.\n",
        "\n",
        "Implement the two methods, one of which writes the stats to file, and the other one of which loads the stats previously written."
      ],
      "metadata": {
        "id": "mEUvC7_ert1w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c2VeOLOfr3Ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0Mpt9FCfRjW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "import string\n",
        "\n",
        "class CollectionStats:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.N = 0 # number of documents\n",
        "        self.TF = 0 # number of tokens in the collection\n",
        "        self.M = 0 # number of unique terms in the collection\n",
        "        self.L = 0 # average length of the documents in the collection\n",
        "        self.C = 0 # average length of the terms in the collection\n",
        "    \n",
        "    def addDocument(self, document):\n",
        "        # update collection statistics based on the new document\n",
        "        self.N += 1\n",
        "        self.TF += len(document)\n",
        "        self.L = ((self.N - 1) * self.L + len(document)) / self.N\n",
        "        \n",
        "        # update the frequency of terms in the collection\n",
        "        terms = set(document)\n",
        "        self.M += len(terms)\n",
        "        term_lengths = sum(len(term) for term in terms)\n",
        "        if self.N == 1:\n",
        "            self.C = term_lengths / len(terms) # initialize C to the average length of terms\n",
        "        else:\n",
        "            self.C = ((self.M - len(terms)) * self.C + term_lengths) / self.M\n",
        "    \n",
        "    def numberOfDocuments(self):\n",
        "        return self.N\n",
        "    \n",
        "    def numberOfTokens(self):\n",
        "        return self.TF\n",
        "    \n",
        "    def numberOfTerms(self):\n",
        "        return self.M\n",
        "    \n",
        "    def averageTokensPerDoc(self):\n",
        "        if self.N != 0:\n",
        "          return self.TF / self.N\n",
        "        else:\n",
        "            return 0\n",
        "    \n",
        "    def averageCharsPerToken(self):\n",
        "        if self.M != 0:\n",
        "          return self.C / self.M\n",
        "        else:\n",
        "          return 0\n",
        "    \n",
        "    def write(self, index_path):\n",
        "        # write collection statistics to disk\n",
        "        with open(os.path.join(index_path, 'collection_stats.pkl'), 'wb') as f:\n",
        "            pickle.dump(self.__dict__, f)\n",
        "    \n",
        "    def load(self, index_path):\n",
        "        # load collection statistics from disk\n",
        "        with open(os.path.join(index_path, 'collection_stats.pkl'), 'rb') as f:\n",
        "            self.__dict__ = pickle.load(f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DJ7SsHQbsAeq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ru-YD1fldulR"
      },
      "source": [
        "## `Lexicon` Class\n",
        "\n",
        "> Implement the `Lexicon` class for **Term Dictionary**\n",
        "\n",
        "> This Lexicon class, for each unique term in the collection, stores \n",
        "1. An integer valued index for the term\n",
        "2. The Document Frequency (DF) for the term<br>\n",
        "\n",
        "That is, it stores $<\\text{term: (idx, DF)}>$ pairs."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BIUHwd03sEE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lusi8UOhebkA"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "class Lexicon:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.lexicon = defaultdict(int) # dictionary containing the terms and their frequency in the collection\n",
        "        self.totalDF = 0 # total document frequency over all terms\n",
        "    \n",
        "    def addToken(self, token):\n",
        "        # add token to the lexicon\n",
        "        self.lexicon[token] += 1\n",
        "        self.totalDF += 1\n",
        "    \n",
        "    def size(self):\n",
        "        return len(self.lexicon)\n",
        "    \n",
        "    def getDF(self, term):\n",
        "        # returns the document frequency of the input term\n",
        "        return self.lexicon.get(term, 0)\n",
        "    \n",
        "    def getTotalDF(self):\n",
        "        # returns the total document frequency over all terms\n",
        "        return sum(self.lexicon.values())\n",
        "    \n",
        "    def getIdx(self, term):\n",
        "        # returns the index of the term in the lexicon\n",
        "        return sorted(list(self.lexicon.keys())).index(term)\n",
        "    \n",
        "    def getTerm(self, idx):\n",
        "        # returns the term for the given index idx\n",
        "        return sorted(list(self.lexicon.keys()))[idx]\n",
        "    \n",
        "    def write(self, index_path):\n",
        "        # write lexicon to disk\n",
        "        with open(os.path.join(index_path, 'lexicon.pkl'), 'wb') as f:\n",
        "            pickle.dump(self.__dict__, f)\n",
        "    \n",
        "    def load(self, index_path):\n",
        "        # load lexicon from disk\n",
        "        with open(os.path.join(index_path, 'lexicon.pkl'), 'rb') as f:\n",
        "            self.__dict__ = pickle.load(f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lPlU8QahsMAp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnuCIn07fvo4"
      },
      "source": [
        "## `Documents` Class\n",
        "\n",
        "> Similar to the `Lexicon` class, `Document` Class, stores, for each of the document in the collection:\n",
        "1. An integer valued index for the document\n",
        "2. Lenght of the document in terms of tokens, i.e. Document Size\n",
        "\n",
        "That is, it stores $<\\text{document: (idx, length)}>$ pairs."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Ufs5R7xsOB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "an7cTKgNhU74"
      },
      "outputs": [],
      "source": [
        "class Documents:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.documents = []\n",
        "        self.doc_idx_map = {}\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self.documents)\n",
        "\n",
        "    def addDocument(self, document):\n",
        "        self.documents.append(document)\n",
        "        self.doc_idx_map[document] = len(self.documents) - 1\n",
        "\n",
        "    def getIdx(self, document):\n",
        "        return self.doc_idx_map.get(document, -1)\n",
        "\n",
        "    def getDocument(self, idx):\n",
        "        if 0 <= idx < len(self.documents):\n",
        "          return self.documents[idx]\n",
        "        return None\n",
        "\n",
        "    def write(self, index_path):\n",
        "        with open(os.path.join(index_path, \"documents.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "            for doc in self:\n",
        "                f.write(doc + \"\\n\")\n",
        "\n",
        "\n",
        "    def load(self, index_path):\n",
        "        with open(os.path.join(index_path, \"documents.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
        "          self.documents = [line.strip() for line in f.readlines()]\n",
        "          self.doc_idx_map = {doc: i for i, doc in enumerate(self.documents)}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bJG7mftMsSbK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osBusvQziKbO"
      },
      "source": [
        "## `Index` Class\n",
        "\n",
        "> This class implements BSB indexer."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HIvbaS9nsVZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7Vk3Nncjia8J"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from collections import Counter\n",
        "import re\n",
        "import json\n",
        "import shutil\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "nltk.download('punkt')  # download punkt tokenizer\n",
        "\n",
        "\n",
        "def get_tokens(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "class Index:\n",
        "    def __init__(self, index_path):\n",
        "        self.index_path = index_path\n",
        "        self.lexicon = Lexicon()\n",
        "        self.documents = Documents()\n",
        "        self.collectionStats = CollectionStats()\n",
        "        self.inverted_index = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    def add_block_to_index(self, block_path, block_id, block_docs, block_tokens):\n",
        "        block = {\n",
        "            'block_id': block_id,\n",
        "            'docs': block_docs,\n",
        "            'tokens': block_tokens\n",
        "        }\n",
        "        block_file = os.path.join(block_path, f'block_{block_id}.json')\n",
        "        with open(block_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(block, f)\n",
        "        return block_file\n",
        "\n",
        "    def flush_block(self, block_id, block_docs, block_tokens):\n",
        "        for doc in block_docs:\n",
        "            self.documents.append(doc)\n",
        "        for token in block_tokens:\n",
        "            self.collectionStats['num_tokens'] += 1\n",
        "            self.collectionStats['total_chars'] += len(token)\n",
        "            self.inverted_index[token][block_id] += 1\n",
        "\n",
        "    def merge_blocks(self, block1, block2):\n",
        "        merged_docs = block1['docs'] + block2['docs']\n",
        "        merged_tokens = block1['tokens'] + block2['tokens']\n",
        "        merged_block = {\n",
        "            'block_id': block1['block_id'],\n",
        "            'docs': merged_docs,\n",
        "            'tokens': merged_tokens\n",
        "        }\n",
        "        return merged_block\n",
        "\n",
        "  \n",
        "    def create(self, collection_path):\n",
        "        # Create Inverted Index using BSB Indexing\n",
        "        BLOCK_SIZE = 10\n",
        "        block_path = os.path.join(self.index_path, \"blocks\")\n",
        "        if os.path.exists(block_path):\n",
        "            shutil.rmtree(block_path)\n",
        "        os.makedirs(block_path)\n",
        "\n",
        "        block_id = 0\n",
        "        block_docs = []\n",
        "        block_tokens = []\n",
        "        block_size = 0\n",
        "\n",
        "        for root, dirs, files in os.walk(collection_path):\n",
        "            for file in files:\n",
        "                if file.endswith(\".txt\"):\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                        content = f.read()\n",
        "                        doc = os.path.relpath(file_path, collection_path)\n",
        "                        block_docs.append(doc)\n",
        "                        tokens = get_tokens(content)\n",
        "                        block_tokens.extend(tokens)\n",
        "                        block_size += len(tokens)\n",
        "\n",
        "                        if block_size > BLOCK_SIZE:\n",
        "                            block_file = self.add_block_to_index(block_path, block_id, block_docs, block_tokens)\n",
        "                            self.flush_block(block_id, block_docs, block_tokens)\n",
        "                            block_docs = []\n",
        "                            block_tokens = []\n",
        "                            block_size = 0\n",
        "                            block_id += 1\n",
        "\n",
        "        if block_size > 0:\n",
        "            block_file = self.add_block_to_index(block_path, block_id, block_docs, block_tokens)\n",
        "            self.flush_block(block_id, block_docs, block_tokens)\n",
        "\n",
        "        # Merge Blocks\n",
        "        block_files = [os.path.join(block_path, f) for f in os.listdir(block_path) if f.endswith(\".json\")]\n",
        "        block_files.sort()\n",
        "   \n",
        "        merged_blocks = []\n",
        "        for block_file in block_files:\n",
        "            with open(block_file, \"r\", encoding=\"utf-8\") as f:\n",
        "                block = json.load(f)\n",
        "                merged_blocks.append(block)\n",
        "        \n",
        "        while len(merged_blocks) > 1:\n",
        "            new_merged_blocks = []\n",
        "            for i in range(0, len(merged_blocks), 2):\n",
        "                if i+1 < len(merged_blocks):\n",
        "                    block1 = merged_blocks[i]\n",
        "                    block2 = merged_blocks[i+1]\n",
        "                    merged_block = self.merge_blocks(block1, block2)\n",
        "                    new_merged_blocks.append(merged_block)\n",
        "                else:\n",
        "                    new_merged_blocks.append(merged_blocks[i])\n",
        "            merged_blocks = new_merged_blocks\n",
        "\n",
        "        # After index is created\n",
        "        self.collectionStats.write(self.index_path)\n",
        "        self.lexicon.write(self.index_path)\n",
        "        self.documents.write(self.index_path)\n",
        "        \n",
        "        if len(merged_blocks) == 1:\n",
        "            self.invertedIndex = merged_blocks[0]\n",
        "            self.write(self.index_path)\n",
        "        else:\n",
        "            print(\"No blocks were created during indexing.\")\n",
        "\n",
        "\n",
        "    def write(self, index_path):\n",
        "        with open(os.path.join(index_path, \"inverted_index.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(self.invertedIndex, f)\n",
        "\n",
        "        with open(os.path.join(index_path, \"documents.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(self.documents.docs, f)\n",
        "\n",
        "    def load(self):\n",
        "      with open(os.path.join(self.index_path, \"inverted_index.json\"), \"r\", encoding=\"utf-8\") as f:\n",
        "          self.invertedIndex = json.load(f)\n",
        "\n",
        "      self.collectionStats.load(self.index_path)\n",
        "      self.lexicon.load(self.index_path)\n",
        "      self.documents.load(self.index_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kIKTjHoNsfki"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "500o_C91lRE-"
      },
      "source": [
        "## Index creation"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JbOQdr9-shjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNdgH-JKkyqI"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "collection_path = \"Your Collection file Path\"\n",
        "index_path = \"Your Indexed file Path\"\n",
        "\n",
        "milliyet_index = Index(index_path)\n",
        "\n",
        "\n",
        "# Create the index using the extracted collection\n",
        "milliyet_index.create(collection_path)\n",
        "\n",
        "# Get collection statistics and lexicon\n",
        "collectionStats = milliyet_index.collectionStats\n",
        "lexicon = milliyet_index.lexicon\n",
        "\n",
        "# Print collection statistics\n",
        "N = collectionStats.numberOfDocuments() # Number of documents\n",
        "L = collectionStats.averageTokensPerDoc() # Average # of tokens per document\n",
        "C = collectionStats.averageCharsPerToken() # Average # of chars per token\n",
        "\n",
        "M = lexicon.size() # Total # of terms, size of term dictionary\n",
        "NPP = lexicon.getTotalDF() # \"Total DF\" over all terms\n",
        "\n",
        "print(\"\\nCollection Statistics\")\n",
        "print(f\"\\nN\\t{N}\\nL\\t{L}\\nC\\t{C}\\nM\\t{M}\\nNPP\\t{NPP}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fn8G7d6ysyMk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQOlJfSClU43"
      },
      "source": [
        "## Index Loading"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gn-LcJBNs1cU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmcLpOjolatD"
      },
      "outputs": [],
      "source": [
        "index_path = \"Your indexed file path\"\n",
        "\n",
        "milliyet_index = Index(index_path)\n",
        "\n",
        "milliyet_index.load();\n",
        "\n",
        "collectionStats = milliyet_index.collectionStats\n",
        "lexicon = milliyet_index.lexicon\n",
        "\n",
        "# Print collection statistics\n",
        "N  = collectionStats.numberOfDocuments() # Number of documents\n",
        "L  = collectionStats.averageTokensPerDoc() # Average # of tokens per document\n",
        "C  = collectionStats.averageCharsPerToken() # Average # of chars per token\n",
        "\n",
        "M  = lexicon.size() # Total # of terms, size of term dictionary\n",
        "NPP = lexicon.getTotalDF() # \"Total DF\" over all terms\n",
        "TF = collectionStats.numberOfTokens() # Total term frequency over all documents\n",
        "print(\"\\nCollection Statistics\")\n",
        "print(f\"\\nN\\t{N}\\nL\\t{L}\\nC\\t{C}\\nM\\t{M}\\nTF\\t{TF}\\nNPP\\t{NPP}\")"
      ]
    }
  ]
}